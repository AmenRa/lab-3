---
title: "Lab 3 - Language Models - Naive"
output: html_notebook
---

# Packages
## Install and import packages

```{r}
# List of packages needed for this tutorial
list.of.packages <- c(
    "NLP",
    "tm",
    "textclean"
)

# Check which packages have not been installed yet
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

# If there are some packages that have not been installed yet, install them
if (length(new.packages)) {
    install.packages(new.packages)
}
```

```{r}
# Import packages
library(data.table)
library("NLP")
library("tm")
library("textclean")
```

# Dataset

## Import Dataset
In this tutorial we use the **Reuters dataset**, which is a well-known dataset composed of several texts and used for **text classification** research.

Reuters is an international business and financial news agency.

The news in the dataset are labelled according to their topics.

The most populated topics are:

1. **acq:** Mergers/Acquisitions;
2. **earn:** Earnings and Earnings Forecasts;
3. **money.fx:** Money/Foreign Exchange.

```{r}
# Import dataset from csv
original_dataset <- fread("reuters_dataset.csv", encoding = 'UTF-8')

# Get the number of rows in dataset
nrow(original_dataset)
```

```{r}
# Print length of the most populated topics (classes) in the dataset
# Since articles can be classified in more than one class we print disjointet sets
cat('topic.acq', length(which(original_dataset$topic.acq == 1
                              & original_dataset$topic.earn == 0
                              & original_dataset$topic.money.fx == 0)), '\n')

cat('topic.earn', length(which(original_dataset$topic.acq == 0
                               & original_dataset$topic.earn == 1
                               & original_dataset$topic.money.fx == 0)), '\n')

cat('topic.money.fx', length(which(original_dataset$topic.acq == 0
                                   & original_dataset$topic.earn == 0
                                   & original_dataset$topic.money.fx == 1)), '\n')
```

## Create a Balanced Dataset

It is a good practice in Machine Learning to use only _balanced_ datasets.
A **balanced dataset** is a dataset composed of an equal number of samples per class.

For this reason we want to compose a new balanced dataset, containing only articles regarding the three most popular topics in the original dataset.

```{r}
# Create a new original_dataset containing only articles related to acq, earn and money.fx
# Each article must belong to a single class
three_class_dataset = original_dataset[
  (original_dataset$topic.acq == 1 & original_dataset$topic.earn == 0 & original_dataset$topic.money.fx == 0)
  | (original_dataset$topic.acq == 0 & original_dataset$topic.earn == 1 & original_dataset$topic.money.fx == 0)
  | (original_dataset$topic.acq == 0 & original_dataset$topic.earn == 0 & original_dataset$topic.money.fx == 1)]
```

```{r}
# Take 800 articles from acq
acq_articles = three_class_dataset[three_class_dataset$topic.acq == 1]
acq_articles = acq_articles[1:800, c('pid', 'doc.text')]
```

```{r}
# Take 800 articles from earn
earn_articles = three_class_dataset[three_class_dataset$topic.earn == 1]
earn_articles = earn_articles[1:800, c('pid', 'doc.text')]
```

```{r}
# Take 800 articles from money
money_articles = three_class_dataset[three_class_dataset$topic.money.fx == 1]
money_articles = money_articles[1:800, c('pid', 'doc.text')]
```


```{r}
# Put the parts togheter, this way the articles are ordered by their class (topic)
dataset <- rbind(acq_articles, earn_articles, money_articles)
```

```{r}
# Rename columns "pid"-> "doc_id", "doc.text" -> "text"
colnames(dataset)[1] <- "doc_id"
colnames(dataset)[2] <- "text"
```

```{r}
# Print dataset number of articles
cat('dataset articles: ', nrow(dataset))
```

# Data Preprocessing
Let's define a function for data preprocessing that can be used when needed.

```{r}
# Data Preprocessing
preprocess_dataset <- function(set) {
  # Tranform the input in a VCorpus (datastructure provided by tm)
  corpus <- VCorpus(DataframeSource(set))
  # Strip white spaces at the beginning and at the end of preprocessing
  # in order to avoid some problems later
  corpus <- tm_map(corpus, content_transformer(stripWhitespace))
  # User replace_contraction function from textclean package
  corpus <- tm_map(corpus, content_transformer(replace_contraction))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, content_transformer(removePunctuation))
  corpus <- tm_map(corpus, content_transformer(removeNumbers))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, content_transformer(stripWhitespace))

  return(corpus)
}
```

## Create Representations

### Feature Selection

```{r}
# Feature selection
apply_feature_selection_on_dtm <- function(dtm_fs, sparsity_value=0.99, verbose=FALSE) {
  if (verbose) {
    print("DTM before sparse term removal")
    inspect(dtm_fs)
  }

  dtm_fs = removeSparseTerms(dtm_fs, sparsity_value)

  if (verbose) {
    print("DTM after sparse term removal")
    inspect(dtm_fs)
  }

  return(dtm_fs)
}
```

```{r}
# Do not remove any word this time!
create_tf_dataframe <- function(corpus, verbose=FALSE) {
  if (verbose) {
    print("Creating tf dataframe...")
  }
  dtm_tf <- DocumentTermMatrix(corpus)
  dataframe_tf <- data.frame(as.matrix(dtm_tf))
  return(dataframe_tf)
}
```

```{r}
create_bigram_tf_dataframe <- function(corpus, sparsity_value = 0.99, verbose = FALSE) {
  if (verbose) {
    print("Creating bigram tf matrix...")
  }
  BigramTokenizer <- function(x) {
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
  }
  dtm_bigram_tf <- DocumentTermMatrix(corpus, control = list(tokenize = BigramTokenizer))
  dtm_bigram_tf <- apply_feature_selection_on_dtm(dtm_bigram_tf, sparsity_value, verbose)
  dataframe_bigram_tf <- data.frame(as.matrix(dtm_bigram_tf))
  return(dataframe_bigram_tf)
}
```

# Dataset Splitting

```{r}
split_dataset <- function(x) {
    # Take 600 acq articles for training set
    train_acq = x[1:600,]
    # Take 200 acq articles for test set
    test_acq = x[601:800,]

    # Take 600 earn articles for training set
    train_earn = x[801:1400,]
    # Take 200 earn articles for test set
    test_earn = x[1401:1600,]

    # Take 600 money articles for training set
    train_money_fx = x[1601:2200,]
    # Take 200 money articles for test set
    test_money_fx = x[2201:2400,]
    
    return(list(train_acq, train_earn, train_money_fx, test_acq, test_earn, test_money_fx))
}
```

# 2 - Define Language Model Functions

A naive method to build a corpus' language model for classification is to compute the probability of each word belonging to the corpus to be generated by it.
This means to calculate the occurrences of a word in the corpus and divide it by the total number of words in the corpus itself.

### Naive Method
```{r}
compute_lm_bigram <- function(bigram_df, unigram_df) {
  # Compute unigram occurrences
  v <- sort(colSums(unigram_df), decreasing = TRUE)
  unigram_occurrences <- data.frame(unigram=names(v),occurrences=v)

  # Compute bigram occurrences
  v <- sort(colSums(bigram_df), decreasing = TRUE)
  bigram_occurrences <- data.frame(bigram=names(v),occurrences=v)

  # Compute probabilities
  probabilities <- transform(
      bigram_occurrences,
      probability=occurrences/unigram_occurrences[strsplit(as.character(bigram), split='[.]')[[1]][1],'occurrences']
  )
  return(probabilities)
}
```

After computing the language model, the probabilities of all the words in the text in exam need to be multiplied together so that we can obtain the likelihood of the text being generated by the language model.

In order to not set the probability to 0 (zero), all the words present in the text but not present in the language model (here they have probability equal to 0) must be skipped.

If no word in the text is present in the language model, then the probability the text has been generated by the language model must be set to 0 (zero).

## Text Generation Probability
```{r}
compute_text_probability <- function(bigram_df, lm) {
  # Initialize the probability of the text being generated by the language model
  p <- 1

  # colnames returns the list of the names of the columns in a dataframe
  # in this case it is the list of bigrams
  for (bigram in colnames(bigram_df)) {

    # if the bigram occurrs at least once in the text in exam,
    # then update the probability of the text being generated by the language model
    if (bigram_df[1, bigram] > 0 & lm[bigram, 'probability'] > 0) {
      p <- p*bigram_df[1, bigram]*lm[bigram, 'probability']
    }
  }

  # If the probability is 1, then we know that
  # no term in the text is present in the language model
  # so we make the probability equal to zero
  if (p == 1) {
    p <- 0
  }

  return(p)
}
```

# Experiments

Now we have all the needed functions to build naive language models and start experimenting.

First of all we have to preprocess the dataset.

```{r}
# Preprocess dataset
preprocessed_dataset <- preprocess_dataset(dataset)
```

Then we have to create a TF dataframe from the dataset (we need term frequencies to compute language models)

- Different level of accuracy can be obtained changing the sparsity value.

```{r}
# Create TF representation
dataset_df <- create_tf_dataframe(preprocessed_dataset, verbose=FALSE)
```

```{r}
# Create Bigram TF dataframe
dataset_bigram_df <- create_bigram_tf_dataframe(preprocessed_dataset, sparsity_value = 0.95)
```

At this point, the dataset can be divided into training and test sets for each topic (we know the rows in the dataframes are ordered by topic).

```{r}
# Split dataset in training and test sets
splitted_dataset = split_dataset(dataset_df)
train_acq = splitted_dataset[[1]]
train_earn = splitted_dataset[[2]]
train_money_fx = splitted_dataset[[3]]
test_acq = splitted_dataset[[4]]
test_earn = splitted_dataset[[5]]
test_money_fx = splitted_dataset[[6]]

# Split dataset in training and test sets
splitted_dataset_bigram = split_dataset(dataset_bigram_df)
train_acq_bigram = splitted_dataset_bigram[[1]]
train_earn_bigram = splitted_dataset_bigram[[2]]
train_money_fx_bigram = splitted_dataset_bigram[[3]]
test_acq_bigram = splitted_dataset_bigram[[4]]
test_earn_bigram = splitted_dataset_bigram[[5]]
test_money_fx_bigram = splitted_dataset_bigram[[6]]
```

Now that we know the frequencies of all words in all texts, we can compute the language models for `acq`, `earn`, and `money_fx`.

```{r}
acq_lm = compute_lm_bigram(train_acq_bigram, train_acq)
earn_lm = compute_lm_bigram(train_earn_bigram, train_earn)
money_fx_lm = compute_lm_bigram(train_money_fx_bigram, train_money_fx)
```

At this point, all we have to do is to calculate the probability of each text to be generated by all the three language models and verify if the probability obtained with the language model of the topic of each text belong to is higher than the other two.

```{r}
acq_results <- c()

# For each test document compute the likelihood it has been generated by each language models
# Put the result in a list in order to check the accuracy later
for(i in 1:nrow(test_acq_bigram)) {
  acq_prob <- compute_text_probability(test_acq_bigram[i, ], acq_lm)
  earn_prob <- compute_text_probability(test_acq_bigram[i, ], earn_lm)
  money_fx_prob <- compute_text_probability(test_acq_bigram[i, ], money_fx_lm)

  if (acq_prob > earn_prob && acq_prob > money_fx_prob) {
    acq_results[[i]] <- TRUE
  } else {
    acq_results[[i]] <- FALSE
  }
}
```

```{r}
earn_results <- c()

for(i in 1:nrow(test_earn_bigram)) {
  acq_prob <- compute_text_probability(test_earn_bigram[i, ], acq_lm)
  earn_prob <- compute_text_probability(test_earn_bigram[i, ], earn_lm)
  money_fx_prob <- compute_text_probability(test_earn_bigram[i, ], money_fx_lm)

  if (earn_prob > acq_prob && earn_prob > money_fx_prob) {
    earn_results[[i]] <- TRUE
  } else {
    earn_results[[i]] <- FALSE
  }
}
```

```{r}
money_fx_results <- c()

for(i in 1:nrow(test_money_fx_bigram)) {
  acq_prob <- compute_text_probability(test_money_fx_bigram[i, ], acq_lm)
  earn_prob <- compute_text_probability(test_money_fx_bigram[i, ], earn_lm)
  money_fx_prob <- compute_text_probability(test_money_fx_bigram[i, ], money_fx_lm)

  if (money_fx_prob > acq_prob && money_fx_prob > earn_prob) {
    money_fx_results[[i]] <- TRUE
  } else {
    money_fx_results[[i]] <- FALSE
  }
}
```

```{r}
# length(which( -condition- )) returns the number of elements in a list
# that satisfy the condition
# In our case we have boolean values in our lists,
# so we can omit the condition and which() will just check
# if the elements have value TRUE
accuracy = (
  length(which(acq_results))
  + length(which(earn_results))
  + length(which(money_fx_results))
) / 600

cat('Bigram language models classification accuracy: ', accuracy)
```



<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
