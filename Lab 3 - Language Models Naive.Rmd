---
title: "Lab 3 - Language Models - Naive"
output: html_notebook
---

# Text Mining & Search - Using Language Models for Text Classification in R

## Step 0 - Install & Import Packages
### Install Packages

```{r}
# install.packages("NLP")
# install.packages("tm")
# install.packages("textclean")
```

### Import Packages

```{r}
library(data.table)
library("tm")
library("NLP")
library("textclean")
```

## Step 1 - Import & Preprocess Data

### Import Dataset

```{r}
# Import dataset from csv
dataset <- fread("reuters_dataset.csv", encoding = 'UTF-8')

# Print length of the most populated topics (classes) in the dataset
cat('topic.acq', length(which(dataset$topic.acq == 1)), '\n')
cat('topic.earn', length(which(dataset$topic.earn == 1)), '\n')
cat('topic.money.fx', length(which(dataset$topic.money.fx == 1)), '\n')
```

```{r}
# Remove every article from the dataset but acq, earn and money.fx
dataset = dataset[
  (dataset$topic.acq == 1 & dataset$topic.earn == 0 & dataset$topic.money.fx == 0)
  | (dataset$topic.acq == 0 & dataset$topic.earn == 1 & dataset$topic.money.fx == 0)
  | (dataset$topic.acq == 0 & dataset$topic.earn == 0 & dataset$topic.money.fx == 1)]
```

```{r}
# Take 800 articles from acq
acq_dataset = dataset[dataset$topic.acq == 1]
acq_dataset_800 = acq_dataset[1:800, c('pid', 'doc.text')]
```

```{r}
# Take 800 articles from earn
earn_dataset = dataset[dataset$topic.earn == 1]
earn_dataset_800 = earn_dataset[1:800, c('pid', 'doc.text')]
```

```{r}
# Take 800 articles from money.fx (we skip one as it causes problems...)
money_fx_dataset = dataset[dataset$topic.money.fx == 1]
money_fx_dataset_800 = rbind(
    money_fx_dataset[1:696, c('pid', 'doc.text')],
    money_fx_dataset[698:801, c('pid', 'doc.text')]
  )
```


```{r}
# Put the parts togheter, this way the articles are ordered by their class (topic)
dataset <- rbind(acq_dataset_800, earn_dataset_800, money_fx_dataset_800)
```

```{r}
# Rename columns
colnames(dataset)[1] <- "doc_id" # from pid
colnames(dataset)[2] <- "text" # from doc.text
```

```{r}
# Print cardinality
cat('dataset cardinality: ', lengths(dataset))
```

### Preprocess Data
It's preferable to define a function for data preprocessing so that it can be reused on occurrence.

```{r}
preprocess_dataset <- function(set) {
  corpus <- VCorpus(DataframeSource(set))
  # Strip white spaces at the beginning and at the end to overcome some problems
  corpus <- tm_map(corpus, content_transformer(stripWhitespace))
  # User replace_contraction function from textclean package
  corpus <- tm_map(corpus, content_transformer(replace_contraction))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, content_transformer(removePunctuation))
  corpus <- tm_map(corpus, content_transformer(removeNumbers))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, content_transformer(stripWhitespace))

  return(corpus)
}
```

```{r}
apply_feature_selection_on_dtm <- function(dtm_fs, sparsity_value = 0.99, verbose = FALSE) {
  if (verbose) {
    print("DTM before sparse term removal")
    inspect(dtm_fs)
  }

  dtm_fs = removeSparseTerms(dtm_fs, sparsity_value)

  if (verbose) {
    print("DTM after sparse term removal")
    inspect(dtm_fs)
  }

  return(dtm_fs)
}
```

```{r}
create_tf_dataframe <- function(corpus, sparsity_value = 0.99, verbose = FALSE) {
  if (verbose) {
    print("Creating tf matrix...")
  }
  dtm_tf <- DocumentTermMatrix(corpus)
  dtm_tf <- apply_feature_selection_on_dtm(dtm_tf, sparsity_value, verbose)

  dataframe_tf <- data.frame(as.matrix(dtm_tf))
  return(dataframe_tf)
}
```

## 2 - Define Language Model Functions

A naive method to build a corpus' language model for classification is to compute the probability of each word belonging to the corpus to be generated by it.
This means to calculate the occurrences of a word in the corpus and divide it by the total number of words in the corpus itself.

### Naive Method
```{r}
compute_lm_naive <- function(df) {
  # Compute occurrences - colSums computes the sum of the elements in each columns (column => term)
  occurrences <- colSums(df)

  # Create a word-occurrences data frame
  # first column = term string
  # second column = number of occurrences of the term
  term_occurrences_df <- data.frame(term=names(occurrences), occurrences)

  # Compute total occurrences (total number of terms)
  total_occurrences <- sum(occurrences)

  # Compute probabilities
  # transform() modifies a dataframe (passed as the first argument)
  # Here we are using transform() in order do add to term_occurrences_df the column "probability"
  # For every row (term) it will be equal to the occurrences of the term divided by
  # the total number of terms in the dataframe
  # probability = occurrences/total_occurrences -> for each row it reads the value of
  # the column occurrences, it divides it by the total number of occurrences and puts
  # the result in a new column called probability
  probabilities <- transform(term_occurrences_df, probability = occurrences/total_occurrences)
  return(probabilities)
}
```

After computing the language model, the probabilities of all the words in the text in exam need to be multiplied together so that we can obtain the likelihood of the text being generated by the language model.

In order to not set the probability to 0 (zero), all the words present in the text but not present in the language model (here they have probability equal to 0) must be skipped.

If no word in the text is present in the language model, then the probability the text has been generated by the language model must be set to 0 (zero).

### Text Generation Probability
```{r}
compute_text_probability <- function(df, lm) {
  # Initialize the probability of the text being generated by the language model
  p <- 1

  # colnames returns the list of the names of the columns in a dataframe
  # in this case it is the list of terms
  for (term in colnames(df)) {

    # if the term occurrs at least once in the text in exam,
    # then update the probability of the text being generated by the language model
    if (df[1, term] > 0) {
      p <- p*df[1, term]*lm[term, 'probability']
    }
  }

  # If the probability is 1, then we know that
  # no term in the text is present in the language model
  # so we make the probability equal to zero
  if (p == 1) {
    p <- 0
  }

  return(p)
}
```

## 3 - Experiments

Now we have all the needed functions to build naive language models and start experimenting.

First of all we have to preprocess the dataset.

```{r}
# Preprocess dataset
preprocessed_dataset <- preprocess_dataset(dataset)
```

Then we have to create a TF dataframe from the dataset (we need term frequencies to compute language models)

- Different level of accuracy can be obtained changing the sparsity value.

```{r}
# Create TF representation
dataset_df <- create_tf_dataframe(preprocessed_dataset, sparsity_value = 0.95)
```

At this point, the dataset can be divided into training and test sets for each topic (we know the rows in the dataframes are ordered by topic).

```{r}
# Acq
acq_df <- dataset_df[1:800,]
train_acq_df <- acq_df[1:600,]
test_acq_df <- acq_df[601:800,]
```

```{r}
# Earn
earn_df <- dataset_df[801:1600,]
train_earn_df <- earn_df[1:600,]
test_earn_df <- earn_df[601:800,]
```

```{r}
# Money.fx
money_fx_df <- dataset_df[1601:2400,]
train_money_fx_df <- money_fx_df[1:600,]
test_money_fx_df <- money_fx_df[601:800,]
```

Now that we know the frequencies of all words in all texts, we can compute the language models for acq, earn, and money_fx.

```{r}
acq_lm = compute_lm_naive(train_acq_df)
earn_lm = compute_lm_naive(train_earn_df)
money_fx_lm = compute_lm_naive(train_money_fx_df)
```

At this point, all we have to do is to calculate the probability of each text to be generated by all the three language models and verify if the probability obtained with the language model of the topic of each text belong to is higher than the other two.

```{r}
acq_results <- c()

# For each test document compute the likelihood it has been generated by each language models
# Put the result in a list in order to check the accuracy later
for(i in 1:nrow(test_acq_df)) {
  acq_prob <- compute_text_probability(test_acq_df[i, ], acq_lm)
  earn_prob <- compute_text_probability(test_acq_df[i, ], earn_lm)
  money_fx_prob <- compute_text_probability(test_acq_df[i, ], money_fx_lm)

  if (acq_prob > earn_prob && acq_prob > money_fx_prob) {
    acq_results[[i]] <- TRUE
  } else {
    acq_results[[i]] <- FALSE
  }
}
```

```{r}
earn_results <- c()

for(i in 1:nrow(test_earn_df)) {
  acq_prob <- compute_text_probability(test_earn_df[i, ], acq_lm)
  earn_prob <- compute_text_probability(test_earn_df[i, ], earn_lm)
  money_fx_prob <- compute_text_probability(test_earn_df[i, ], money_fx_lm)

  if (earn_prob > acq_prob && earn_prob > money_fx_prob) {
    earn_results[[i]] <- TRUE
  } else {
    earn_results[[i]] <- FALSE
  }
}
```

```{r}
money_fx_results <- c()

for(i in 1:nrow(test_money_fx_df)) {
  acq_prob <- compute_text_probability(test_money_fx_df[i, ], acq_lm)
  earn_prob <- compute_text_probability(test_money_fx_df[i, ], earn_lm)
  money_fx_prob <- compute_text_probability(test_money_fx_df[i, ], money_fx_lm)

  if (money_fx_prob > acq_prob && money_fx_prob > earn_prob) {
    money_fx_results[[i]] <- TRUE
  } else {
    money_fx_results[[i]] <- FALSE
  }
}
```

```{r}
# length(which( -condition- )) returns the number of elements in a list
# that satisfy the condition
# In our case we have boolean values in our lists,
# so we can omit the condition and which() will just check
# if the elements have value TRUE
accuracy = (
  length(which(acq_results))
  + length(which(earn_results))
  + length(which(money_fx_results))
) / 600

cat('Naive language models classification accuracy: ', accuracy)
```

<h4 style="color:red">EXERCISE 1: for each class count the number of misclassificated elements for each other class</h4>

<h4 style="color:red">EXERCISE 2: compute the similirity between the language models in order to check why the misclassification happens</h4>

**HINT:** use the cosine similarity (install package "lsa", use the cosine() function - it takes two vectors as arguments!) - you can also code it by yourself if!

<h4 style="color:red">EXERCISE 3: build bigram language models and do the same experiment</h4>

**HINT:** (look below)
```{r}
probabilities <- transform(bigram_occurrencies, probability=occurrencies/unigram_occurrencies[strsplit(as.character(bigram), split = '[.]')[[1]][1], 'occurrencies'])
```

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
